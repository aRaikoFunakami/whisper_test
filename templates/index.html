<!DOCTYPE html>
<html>
<head>
    <title>音声テキスト化</title>
    <script defer src="./hark.bundle.js"></script>
</head>
<body>
    <h1>音声テキスト化</h1>
    <img id="toggleRecording" src="mic_ready.png" data-state="inactive" style="cursor: pointer;">
    <div id="transcription"></div>
    <script>
        const toggleButton = document.getElementById('toggleRecording');
        const transcriptionDiv = document.getElementById('transcription');
        let mediaRecorder;
        let audioChunks = [];
        let silenceTimer;
        const SILENCE_DURATION = 1000; // 無音継続時間（ミリ秒）

        toggleButton.addEventListener('click', async () => {
            const currentState = toggleButton.getAttribute('data-state');

            if (currentState === 'inactive') {
                const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
                /*
                Speech recognition ends when silence continues for a certain period of time
                */
                const options = {
                    threshold: -50,
                    interval: 150
                };
                const speechEvents = hark(stream, options);
                speechEvents.on('stopped_speaking', function() {
                    console.log('stopped_speaking');
                    silenceTimer = setTimeout(() => {
                        clearTimeout(silenceTimer);
                        mediaRecorder.stop();
                        toggleButton.src = 'mic_disable.png';
                        toggleButton.style.cursor = 'not-allowed';
                    }, SILENCE_DURATION);
                    speechEvents.off('stopped_speaking');
                });

                speechEvents.on('speaking', function() {
                    console.log('speaking');
                    clearTimeout(silenceTimer);
                });

                /*
                Recording audio
                */
                mediaRecorder = new MediaRecorder(stream);

                mediaRecorder.ondataavailable = (event) => {
                    if (event.data.size > 0) {
                        audioChunks.push(event.data);
                    }
                };

                mediaRecorder.onstop = async () => {
                    // Create recorded data as a Blob
                    const audioBlob = new Blob(audioChunks, { type: 'audio/wav' });
                    const formData = new FormData();
                    formData.append('audio', audioBlob);

                    const response = await fetch('/upload-audio', {
                        method: 'POST',
                        body: formData,
                    });

                    const data = await response.json();
                    console.log(data);

                    // Display response in HTML element
                    transcriptionDiv.innerHTML = data.transcription;

                    // Other post-processing (e.g., update button status)
                    toggleButton.src = 'mic_ready.png';
                    toggleButton.setAttribute('data-state', 'inactive');
                    toggleButton.style.cursor = 'pointer';
                };

                toggleButton.src = 'mic_recording.png';
                toggleButton.setAttribute('data-state', 'active');
                audioChunks = [];
                mediaRecorder.start();
            } else if (currentState === 'active') {
                /*
                Click to end voice recognition
                */
                mediaRecorder.stop();
                toggleButton.src = 'mic_disable.png';
                toggleButton.style.cursor = 'not-allowed';
            }
        });
    </script>
</body>
</html>
